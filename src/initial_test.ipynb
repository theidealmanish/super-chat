{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.core import Root\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# service parameters\n",
    "CONNECTION_PARAMS = {\n",
    "    \"account\": os.environ.get(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    \"user\": os.environ.get(\"SNOWFLAKE_USER\"),\n",
    "    \"password\": os.environ.get(\"SNOWFLAKE_USER_PASSWORD\"),\n",
    "    \"role\": os.environ.get(\"SNOWFLAKE_ROLE\"),\n",
    "    \"database\": os.environ.get(\"SNOWFLAKE_DATABASE\"),\n",
    "    \"schema\": os.environ.get(\"SNOWFLAKE_SCHEMA\"),\n",
    "    \"warehouse\": os.environ.get(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    \"search_service\": os.environ.get(\"SNOWFLAKE_CORTEX_SEARCH_SERVICE\"),\n",
    "}\n",
    "\n",
    "\n",
    "SESSION = Session.builder.configs(CONNECTION_PARAMS).create()\n",
    "SVC = Root(SESSION).databases[CONNECTION_PARAMS[\"database\"]].schemas[CONNECTION_PARAMS[\"schema\"]\n",
    "                                                                     ].cortex_search_services[CONNECTION_PARAMS[\"search_service\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Groundedness, input source will be set to __record__.app.retrieve_context.rets[:].collect() .\n",
      "âœ… In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In Context Relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Context Relevance, input context will be set to __record__.app.retrieve_context.rets[:] .\n",
      "âœ… In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "from trulens.providers.cortex.provider import Cortex\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import Select\n",
    "import numpy as np\n",
    "\n",
    "provider = Cortex(snowpark_session=SESSION, model_engine=\"mistral-large2\")\n",
    "\n",
    "f_groundedness = (\n",
    "    Feedback(\n",
    "        provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\")\n",
    "    .on(Select.RecordCalls.retrieve_context.rets[:].collect())\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "f_context_relevance = (\n",
    "    Feedback(\n",
    "        provider.context_relevance,\n",
    "        name=\"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on(Select.RecordCalls.retrieve_context.rets[:])\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "f_answer_relevance = (\n",
    "    Feedback(\n",
    "        provider.relevance,\n",
    "        name=\"Answer Relevance\")\n",
    "    .on_input()\n",
    "    .on_output()\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "feedbacks = [f_context_relevance,\n",
    "             f_answer_relevance,\n",
    "             f_groundedness,\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Snowflakes get their unique patterns through a complex process that involves both physics and chemistry. It all starts with a tiny particle in the atmosphere, like a dust or pollen grain, which serves as a nucleus for the snowflake to form around.\n",
      "\n",
      "As this particle cools, water vapor in the air begins to condense and freeze onto it, forming an ice crystal. The shape of this initial crystal is determined by the arrangement of water molecules, which naturally form a hexagonal structure due to the hydrogen bonds between them.\n",
      "\n",
      "As the ice crystal falls through the atmosphere, it encounters different temperatures and levels of humidity. These varying conditions cause the crystal to grow in a unique way, with intricate branches and patterns forming as more water molecules attach to it.\n",
      "\n",
      "The six-sided symmetry of snowflakes comes from the hexagonal structure of water molecules. However, the exact pattern of each snowflake is influenced by the specific path it takes through the atmosphere and the conditions it experiences along the way. This is why no two snowflakes are exactly alike.\n",
      "\n",
      "In essence, the unique patterns of snowflakes are a result of the interplay between the inherent structure of water molecules and the variable conditions in the atmosphere.\n"
     ]
    }
   ],
   "source": [
    "from snowflake.cortex import complete\n",
    "\n",
    "print(complete(\"mistral-large\",\n",
    "      \"how do snowflakes get their unique patterns?\", session=SESSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running the TruLens dashboard requires providing a `password` to the `SnowflakeConnector`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Initialized with db url snowflake://BEYOND:***@wqb64360/SUPER_CHAT/DATA?role=ACCOUNTADMIN&warehouse=COMPUTE_WH .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `TruSession` to prevent this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Singleton instance TruSession already exists for name = None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set TruLens workspace version tag: [('Statement executed successfully.',)]\n"
     ]
    }
   ],
   "source": [
    "from trulens.core import TruSession\n",
    "from trulens.connectors.snowflake import SnowflakeConnector\n",
    "\n",
    "tru_snowflake_connector = SnowflakeConnector(snowpark_session=SESSION)\n",
    "\n",
    "tru_session = TruSession(connector=tru_snowflake_connector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decorating <function RAG_from_scratch.retrieve_context at 0x30a3f4280>\n",
      "decorating <function RAG_from_scratch.generate_completion at 0x30a3f4430>\n",
      "decorating <function RAG_from_scratch.query at 0x30a3f4550>\n",
      "adding method <class '__main__.RAG_from_scratch'> retrieve_context __main__\n",
      "adding method <class '__main__.RAG_from_scratch'> generate_completion __main__\n",
      "adding method <class '__main__.RAG_from_scratch'> query __main__\n"
     ]
    }
   ],
   "source": [
    "from trulens.apps.custom import instrument\n",
    "import json\n",
    "\n",
    "\n",
    "class RAG_from_scratch:\n",
    "    def __init__(self, bot_id=\"902\", model_name=\"mistral-large2\", num_chunks=5, session=SESSION):\n",
    "        \"\"\"\n",
    "        Initialize the Chat Assistant.\n",
    "        \"\"\"\n",
    "        self.bot_id = bot_id\n",
    "        self.model_name = model_name\n",
    "        self.num_chunks = num_chunks\n",
    "        self.columns = [\"chunk_text\", \"source_url\", \"bot_id\"]\n",
    "        self.session = session\n",
    "\n",
    "    @instrument\n",
    "    def retrieve_context(self, query):\n",
    "        \"\"\"\n",
    "        Retrieve relevant text chunks from the database using Cortex Search.\n",
    "        \"\"\"\n",
    "        filter_obj = {\"@eq\": {\"bot_id\": self.bot_id}}\n",
    "        response = SVC.search(\n",
    "            query, self.columns, filter=filter_obj, limit=self.num_chunks, session=self.session)\n",
    "        return json.loads(response.model_dump_json())\n",
    "\n",
    "    @instrument\n",
    "    def generate_completion(self, query, context_str, bot_info, chat_history=None):\n",
    "        \"\"\"\n",
    "        Generate a response to the query using the provided context and chat history.\n",
    "        \"\"\"\n",
    "        if chat_history is None:\n",
    "            chat_history = []\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are an expert chat assistant, you can find your identity in between <identity> and </identity> (That's what you call yourself),\n",
    "        that extracts information from the CONTEXT provided\n",
    "        between <context> and </context> tags.\n",
    "        You offer a chat experience considering the information included in the CHAT HISTORY\n",
    "        provided between <chat_history> and </chat_history> tags.\n",
    "        \n",
    "        When answering the question contained between <question> and </question> tags,\n",
    "        be concise and do not hallucinate. Don't add your belief, make it more relevant to\n",
    "        the context provided.\n",
    "\n",
    "        If you donâ€™t have the information, say you don't have that information, or you don't know.\n",
    "        Don't say you are just an AI or AI assistant and explain. Keep it clear. \n",
    "        Don't let the user know that you are an AI, or AI assistant.\n",
    "\n",
    "        Do not mention the CONTEXT used in your answer.\n",
    "        Do not mention the CHAT HISTORY used in your answer.\n",
    "        Make the answer DESCRIPTIVE, and EXPLAINING, don't make it vague.\n",
    "\n",
    "        If you are to generate code, make it same as they are, as they\n",
    "        are the official documentation of the project. Make it real, and exact.\n",
    "        \n",
    "        Provide all the sources as Source: with relevant URLs of the context you are using, that are clickable. \n",
    "        Only answer the question if you can extract it from the CONTEXT provided.\n",
    "        \n",
    "        <identity>\n",
    "        Name: {bot_info['NAME']}\n",
    "        Description: {bot_info['DESCRIPTION']}\n",
    "        </identity> \n",
    "        <chat_history>\n",
    "        {chat_history}\n",
    "        </chat_history>\n",
    "        <context>          \n",
    "        {context_str}\n",
    "        </context>\n",
    "        <question>  \n",
    "        {query}\n",
    "        </question>\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "        # Generate response using the LLM\n",
    "        response = complete(self.model_name, prompt, session=self.session)\n",
    "        return response\n",
    "\n",
    "    @instrument\n",
    "    def query(self, query, bot_info={\"NAME\": \"Mark Manson\", \"DESCRIPTION\": \"This is mark.\"}):\n",
    "        \"\"\"\n",
    "        Perform a complete query by retrieving relevant chunks and generating a response.\n",
    "        \"\"\"\n",
    "        # Step 1: Retrieve context\n",
    "        retrieved_context = self.retrieve_context(query)\n",
    "\n",
    "        # Step 2: Generate response\n",
    "        response = self.generate_completion(query, retrieved_context, bot_info)\n",
    "        return response\n",
    "\n",
    "\n",
    "rag = RAG_from_scratch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" To maintain a healthy relationship, it's important to understand that each individual is responsible for their own happiness. It's not your partner's job to make you happy; rather, both of you should figure out what makes you happy as individuals and bring that happiness into the relationship.\\n\\nAdditionally, having open and honest conversations is crucial for maintaining a healthy relationship. These conversations help both partners understand each other's needs and prevent losing track of one another.\\n\\nTrust is also a key component. Trust your partner to go off on their own and donâ€™t get insecure or angry if you see them talking with someone else.\\n\\nLastly, be passionate about shared responsibilities like cleaning the house and preparing meals. Do these tasks together and make them fun. Avoid complaining about your partner to others, and always give each other the benefit of the doubt. Have a life outside of each other but share it through conversation.\\n\\nSource: [Mark Manson - Relationship Advice](https://markmanson.net/relationship-advice)\\nSource: [Mark Manson - Healthy Relationship Habits](https://markmanson.net/healthy-relationship-habits)\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.query(\"How to maintain healthy relationship?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Groundedness, input source will be set to __record__.app.retrieve_context.rets[:].collect() .\n",
      "âœ… In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In Context Relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Context Relevance, input context will be set to __record__.app.retrieve_context.rets[:] .\n",
      "âœ… In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "from trulens.providers.cortex.provider import Cortex\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import Select\n",
    "import numpy as np\n",
    "\n",
    "provider = Cortex(SESSION, model_engine=\"mistral-large2\")\n",
    "\n",
    "f_groundedness = (\n",
    "    Feedback(provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\")\n",
    "    .on(Select.RecordCalls.retrieve_context.rets[:].collect())\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "f_context_relevance = (\n",
    "    Feedback(provider.context_relevance, name=\"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on(Select.RecordCalls.retrieve_context.rets[:])\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "f_answer_relevance = (\n",
    "    Feedback(provider.relevance, name=\"Answer Relevance\")\n",
    "    .on_input()\n",
    "    .on_output()\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instrumenting <class '__main__.RAG_from_scratch'> for base <class '__main__.RAG_from_scratch'>\n",
      "\tinstrumenting retrieve_context\n",
      "\tinstrumenting generate_completion\n",
      "\tinstrumenting query\n",
      "skipping base <class 'object'> because of class\n"
     ]
    }
   ],
   "source": [
    "from trulens.apps.custom import TruCustomApp\n",
    "\n",
    "tru_rag = TruCustomApp(\n",
    "    rag,\n",
    "    app_name=\"SUPER_CHAT\",\n",
    "    app_version=\"simple\",\n",
    "    feedbacks=[f_groundedness, f_answer_relevance, f_context_relevance],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"What are five healthy relationship habits?\",\n",
    "    \"How do I communicate better with my partner?\",\n",
    "    \"Whatâ€™s the best way to resolve conflicts in a relationship?\",\n",
    "    \"How can I rebuild trust after a disagreement?\",\n",
    "    \"What are some ways to show appreciation in a relationship?\",\n",
    "    \"How can I balance personal space and togetherness in a relationship?\",\n",
    "    \"What are the signs of a healthy vs. unhealthy relationship?\",\n",
    "    \"How do I set boundaries in a relationship without causing issues?\",\n",
    "    \"What are some thoughtful date ideas to strengthen our bond?\",\n",
    "    \"How can we keep the spark alive in a long-term relationship?\",\n",
    "    \"What are the red flags to watch out for in a partner?\",\n",
    "    \"How do I navigate differences in love languages?\",\n",
    "    \"What role does trust play in building a strong relationship?\",\n",
    "    \"How can I help my partner feel heard and understood?\",\n",
    "    \"What are the best practices for managing finances as a couple?\",\n",
    "    \"How do I handle jealousy or insecurity in a relationship?\",\n",
    "    \"What are some ways to improve communication in a relationship?\",\n",
    "    \"How can I support my partner in overcoming personal challenges?\",\n",
    "    \"What are the best ways to celebrate our anniversary?\",\n",
    "    \"How do I handle the unexpected in a relationship?\",\n",
    "    \"What are some ways to strengthen our family bond in a relationship?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_name</th>\n",
       "      <th>app_version</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SUPER_CHAT</th>\n",
       "      <th>simple</th>\n",
       "      <td>0.97561</td>\n",
       "      <td>6.084832</td>\n",
       "      <td>0.446553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Answer Relevance   latency  total_cost\n",
       "app_name   app_version                                        \n",
       "SUPER_CHAT simple                0.97561  6.084832    0.446553"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tru_rag as recording:\n",
    "    for prompt in prompts:\n",
    "        rag.query(prompt)\n",
    "\n",
    "tru_session.get_leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decorating <function context_filter.__call__.<locals>.wrapper at 0x311ba80d0>\n",
      "adding method <class '__main__.filtered_RAG_from_scratch'> retrieve_context __main__\n"
     ]
    }
   ],
   "source": [
    "from trulens.core.guardrails.base import context_filter\n",
    "\n",
    "# note: feedback function used for guardrail must only return a score, not also reasons\n",
    "f_context_relevance_score = Feedback(\n",
    "    provider.context_relevance, name=\"Context Relevance\"\n",
    ")\n",
    "\n",
    "\n",
    "class filtered_RAG_from_scratch(RAG_from_scratch):\n",
    "\n",
    "    @instrument\n",
    "    @context_filter(f_context_relevance_score, 0.75, keyword_for_prompt=\"query\")\n",
    "    def retrieve_context(self, query: str) -> list:\n",
    "        \"\"\"\n",
    "        Retrieve relevant text from vector store.\n",
    "        \"\"\"\n",
    "        return self.retriever.retrieve(query)\n",
    "\n",
    "\n",
    "filtered_rag = filtered_RAG_from_scratch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Object (of type list is a sequence containing more than one dictionary. Lookup by item or attribute `rets` is ambiguous. Use a lookup by index(es) or slice first to disambiguate.\n",
      "Object (of type list is a sequence containing more than one dictionary. Lookup by item or attribute `rets` is ambiguous. Use a lookup by index(es) or slice first to disambiguate.\n",
      "Object (of type list is a sequence containing more than one dictionary. Lookup by item or attribute `rets` is ambiguous. Use a lookup by index(es) or slice first to disambiguate.\n",
      "Object (of type list is a sequence containing more than one dictionary. Lookup by item or attribute `rets` is ambiguous. Use a lookup by index(es) or slice first to disambiguate.\n",
      "Object (of type list is a sequence containing more than one dictionary. Lookup by item or attribute `rets` is ambiguous. Use a lookup by index(es) or slice first to disambiguate.\n",
      "Object (of type list is a sequence containing more than one dictionary. Lookup by item or attribute `rets` is ambiguous. Use a lookup by index(es) or slice first to disambiguate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instrumenting <class '__main__.filtered_RAG_from_scratch'> for base <class '__main__.filtered_RAG_from_scratch'>\n",
      "\tinstrumenting retrieve_context\n",
      "\tinstrumenting generate_completion\n",
      "\tinstrumenting query\n",
      "instrumenting <class '__main__.filtered_RAG_from_scratch'> for base <class '__main__.RAG_from_scratch'>\n",
      "\tinstrumenting retrieve_context\n",
      "\tinstrumenting generate_completion\n",
      "\tinstrumenting query\n",
      "skipping base <class 'object'> because of class\n"
     ]
    }
   ],
   "source": [
    "# check if we have better performance\n",
    "from trulens.apps.custom import TruCustomApp\n",
    "\n",
    "tru_filtered_rag = TruCustomApp(\n",
    "    filtered_rag,\n",
    "    app_name=\"RAG\",\n",
    "    app_version=\"filtered\",\n",
    "    feedbacks=[f_groundedness, f_answer_relevance, f_context_relevance],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_name</th>\n",
       "      <th>app_version</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SUPER_CHAT</th>\n",
       "      <th>simple</th>\n",
       "      <td>0.97561</td>\n",
       "      <td>6.084832</td>\n",
       "      <td>0.446553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Answer Relevance   latency  total_cost\n",
       "app_name   app_version                                        \n",
       "SUPER_CHAT simple                0.97561  6.084832    0.446553"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tru_rag as recording:\n",
    "    for prompt in prompts:\n",
    "        rag.query(prompt)\n",
    "\n",
    "tru_session.get_leaderboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
